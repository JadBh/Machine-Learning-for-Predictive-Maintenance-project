{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:06:29.627120900Z",
     "start_time": "2023-11-15T10:06:29.610425800Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "from dataclasses import dataclass\n",
    "import hvplot.pandas \n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# hv.renderer('bokeh').theme = 'dark_minimal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_root = Path(r\"C:\\Users\\Raffael\\Documents\\Datasets\\alpiq_2023\") # Raw string works without escaping \\\n",
    "# dataset_root = Path(r\"C:/Users/jadbh\\Documents/Swisse/EPFL/courses/Fall 2024/Machine Learning for Predictive Maintenance/project/Dataset\")\n",
    "# dataset_root = Path(r\"C:\\Users\\jadbh\\Documents\\Swisse\\EPFL\\courses\\Fall 2024\\Machine Learning for Predictive Maintenance\\project\\team repo\\Machine-Learning-for-Predictive-Maintenance-project\\Dataset\")\n",
    "dataset_root = Path(r'Dataset')\n",
    "\n",
    "@dataclass\n",
    "class Case():\n",
    "    info: pd.DataFrame\n",
    "    measurements: pd.DataFrame\n",
    "\n",
    "\n",
    "class RawDataset():\n",
    "\n",
    "    def __init__(self, root, unit = \"VG4\", load_training=False, load_synthetic=False) -> None:\n",
    "        \n",
    "        \n",
    "        read_pq_file = lambda f: pq.read_table(root / f).to_pandas()\n",
    "        \n",
    "        \n",
    "        cases = {\n",
    "            \"test\": [f\"{unit}_generator_data_testing_real_measurements.parquet\", root / f\"{unit}_generator_data_testing_real_info.csv\" ], \n",
    "        }\n",
    "        \n",
    "        if load_training:\n",
    "            cases = {\n",
    "                **cases,\n",
    "                \"train\": [f\"{unit}_generator_data_training_measurements.parquet\", root / f\"{unit}_generator_data_training_info.csv\" ], \n",
    "            }\n",
    "        \n",
    "        if load_synthetic:\n",
    "            cases = {\n",
    "                **cases,\n",
    "                \"test_s01\": [f\"{unit}_generator_data_testing_synthetic_01_measurements.parquet\", root / f\"{unit}_generator_data_testing_synthetic_01_info.csv\"], \n",
    "                \"test_s02\": [f\"{unit}_generator_data_testing_synthetic_02_measurements.parquet\", root / f\"{unit}_generator_data_testing_synthetic_02_info.csv\"]\n",
    "            }\n",
    "        \n",
    "        \n",
    "        self.data_dict = dict()\n",
    "        \n",
    "        for id_c, c in cases.items():\n",
    "            # if you need to verify the parquet header:\n",
    "            # pq_rows = RawDataset.read_parquet_schema_df(root / c[0])\n",
    "            info = pd.read_csv(c[1])\n",
    "            measurements = read_pq_file(c[0])\n",
    "            self.data_dict[id_c] = Case(info, measurements)\n",
    "            \n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def read_parquet_schema_df(uri: str) -> pd.DataFrame:\n",
    "        \"\"\"Return a Pandas dataframe corresponding to the schema of a local URI of a parquet file.\n",
    "\n",
    "        The returned dataframe has the columns: column, pa_dtype\n",
    "        \"\"\"\n",
    "        # Ref: https://stackoverflow.com/a/64288036/\n",
    "        schema = pq.read_schema(uri, memory_map=True)\n",
    "        schema = pd.DataFrame(({\"column\": name, \"pa_dtype\": str(pa_dtype)} for name, pa_dtype in zip(schema.names, schema.types)))\n",
    "        schema = schema.reindex(columns=[\"column\", \"pa_dtype\"], fill_value=pd.NA)  # Ensures columns in case the parquet file has an empty dataframe.\n",
    "        return schema\n",
    "    \n",
    "\n",
    "rds_u4 = RawDataset(dataset_root, \"VG4\", load_synthetic=False, load_training=True)\n",
    "rds_u5 = RawDataset(dataset_root, \"VG5\", load_synthetic=True, load_training=True)\n",
    "rds_u6 = RawDataset(dataset_root, \"VG6\", load_synthetic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg5_train_meas = rds_u5.data_dict[\"train\"].measurements\n",
    "vg5_train_info = rds_u5.data_dict[\"train\"].info\n",
    "vg5_test_meas = rds_u5.data_dict[\"test\"].measurements\n",
    "vg5_test_info = rds_u5.data_dict[\"test\"].info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg5_train_filt_turbine = vg5_train_meas [ (vg5_train_meas['equilibrium_turbine_mode'] == True)] \n",
    "                                        #    ((vg5_train_meas['equilibrium_pump_mode'] == True) & (vg5_train_meas['short_circuit_mode'] == False)) |\n",
    "                                        #    ((vg5_train_meas['equilibrium_pump_mode'] == True) & (vg5_train_meas['short_circuit_mode'] == True) & (vg5_train_meas['equilibrium_short_circuit_mode'] == True)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg5_s1 = rds_u5.data_dict[\"test_s01\"].measurements          # synthetic testing\n",
    "vg5_s1_filt_turbine = vg5_s1 [ (vg5_s1['equilibrium_turbine_mode'] == True) ]        # filtered synthetic testing\n",
    "                    # ((vg5_s1['equilibrium_pump_mode'] == True) & (vg5_s1['short_circuit_mode'] == False)) |\n",
    "                    # ((vg5_s1['equilibrium_pump_mode'] == True) & (vg5_s1['short_circuit_mode'] == True) & (vg5_s1['equilibrium_short_circuit_mode'] == True)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg5_train_meas.reset_index(inplace=True)\n",
    "vg5_train_filt_turbine = vg5_train_meas [ (vg5_train_meas['equilibrium_turbine_mode'] == True) ]\n",
    "                                        #    ((vg5_train_meas['equilibrium_pump_mode'] == True) & (vg5_train_meas['short_circuit_mode'] == False)) |\n",
    "                                        #    ((vg5_train_meas['equilibrium_pump_mode'] == True) & (vg5_train_meas['short_circuit_mode'] == True) & (vg5_train_meas['equilibrium_short_circuit_mode'] == True)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # summary of VG5 useful variables\n",
    "\n",
    "# vg5_train_info\n",
    "# vg5_train_filt      # equilibirum\n",
    "# vg5_train_meas\n",
    "\n",
    "# vg5_test_info\n",
    "# vg5_test_meas\n",
    "\n",
    "# vg5_s1\n",
    "# vg5_s1_filt         # equilibrium\n",
    "\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = vg5_train_filt_turbine\n",
    "print(df.columns)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"OG columns: \", df.columns.to_list())\n",
    "print(\"OG columns length: \", len(df.columns.to_list()))\n",
    "df.drop(columns=df.loc[:, 'machine_on':'equilibrium_short_circuit_mode'].columns, inplace=True)\n",
    "df.drop(['index'], axis = 1, inplace=True)\n",
    "print(\"final columns:       \", df.columns.to_list())\n",
    "print(\"final columns lengthL\", len(df.columns.to_list()))\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowDataset(Dataset):\n",
    "    def __init__(self, dataframe, feature_columns, window_size, step_size=1, max_gap=10, stride = 2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): The dataframe containing sensor data.\n",
    "            feature_columns (list): List of column names for features.\n",
    "            window_size (int): The number of timesteps in each sliding window.\n",
    "            step_size (int): The step size to slide the window.\n",
    "            max_gap (int): Maximum allowed gap between consecutive indices for grouping.\n",
    "        \"\"\"\n",
    "        self.features = dataframe[feature_columns].values\n",
    "        self.indices = dataframe.index.values\n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "        self.max_gap = max_gap\n",
    "        self.stride = stride\n",
    "\n",
    "        # Identify groups based on index gaps\n",
    "        self.groups = self._identify_groups()\n",
    "        # print(self.groups[0:50])\n",
    "        # print(len(self.groups[500]))\n",
    "        self.valid_windows = self._generate_valid_windows()\n",
    "\n",
    "    def _identify_groups(self):\n",
    "        \"\"\"\n",
    "        Identify groups of rows based on the max_gap condition.\n",
    "        \"\"\"\n",
    "        groups = []\n",
    "        current_group = [0]  # Start with the first row\n",
    "        for i in range(1, len(self.indices)):\n",
    "            if self.indices[i] - self.indices[i - 1] > self.max_gap:\n",
    "                groups.append(current_group)\n",
    "                current_group = [i]\n",
    "            else:\n",
    "                current_group.append(i)\n",
    "        groups.append(current_group)  # Add the last group\n",
    "        return groups\n",
    "\n",
    "    def _generate_valid_windows(self):\n",
    "        \"\"\"\n",
    "        Generate valid sliding windows based on groups.\n",
    "        \"\"\"\n",
    "        valid_windows = []\n",
    "        for group in self.groups:\n",
    "            \n",
    "            for start in range(0, len(group) - self.window_size + 1, self.stride):\n",
    "                valid_windows.append(group[start : start + self.window_size])\n",
    "\n",
    "            if (len(group) - self.window_size)%self.stride != 0 and len(group)> self.window_size:\n",
    "                # print(len(group[len(group) - self.window_size : len(group)]))\n",
    "                valid_windows.append(group[len(group) - self.window_size : len(group)])\n",
    "\n",
    "        # print(valid_windows[-1])\n",
    "        return valid_windows\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a sliding window by index.\n",
    "        \"\"\"\n",
    "        window_indices = self.valid_windows[idx]\n",
    "        x = torch.tensor(self.features[window_indices], dtype=torch.float32)\n",
    "        return x\n",
    "\n",
    "feature_columns = df.columns\n",
    "# feature_columns = [\"charge\"]\n",
    "# feature_columns = df.columns.to_string()\n",
    "window_size = 100\n",
    "stride = 10\n",
    "max_gap = 10\n",
    "batch_size = 32\n",
    "step_size = 1  # Step size for sliding window\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle = False)\n",
    "print(\"Total df shape:\", df.shape)\n",
    "print(\"Training set shape:\", train_df.shape)\n",
    "print(\"Testing set shape:\", test_df.shape)\n",
    "\n",
    "\n",
    "# Initialize a scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler.fit(train_df[feature_columns])\n",
    "\n",
    "# Transform the training and testing data\n",
    "train_df[feature_columns] = scaler.transform(train_df[feature_columns])\n",
    "test_df[feature_columns] = scaler.transform(test_df[feature_columns])\n",
    "\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = SlidingWindowDataset(train_df, feature_columns, window_size, step_size, max_gap, stride = stride)\n",
    "test_dataset = SlidingWindowDataset(test_df, feature_columns, window_size, step_size, max_gap, stride = stride)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"train dataloader length: \", len(train_dataloader))\n",
    "print(\"train dataloader dataset length: \", len(train_dataloader.dataset))\n",
    "\n",
    "print(\"test dataloader length: \", len(test_dataloader))\n",
    "print(\"test dataloader dataset length: \", len(test_dataloader.dataset))\n",
    "\n",
    "# maxes = []\n",
    "# # Example: Iterating through the dataloader\n",
    "# for i, batch in enumerate(dataloader):\n",
    "#     sliding_windows = batch\n",
    "#     diff = sliding_windows[0,1:10,1] - sliding_windows[0,0:9,1]\n",
    "#     maxes.append(diff.max().item())\n",
    "#     # print(maxes)  # Replace with your training loop logi\n",
    "#     # if i>100:\n",
    "#     #     break\n",
    "# # plt.hist(maxes)\n",
    "# pd_maxes = pd.DataFrame(maxes)\n",
    "# pd_maxes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseAutoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(DenseAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            )\n",
    "                          \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_dim)\n",
    "            # nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature = self.encoder(x)\n",
    "        reconstruction = self.decoder(feature)\n",
    "        return reconstruction, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, print_every=10):\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for _ , x in enumerate(tqdm(dataloader)):\n",
    "        batch_size = x.shape[0]\n",
    "        total_samples += batch_size\n",
    "        \n",
    "        x = x.flatten(start_dim=1)\n",
    "        optimizer.zero_grad()\n",
    "        x_pred, _ = model(x)\n",
    "        \n",
    "        loss = loss_fn(x_pred, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * batch_size\n",
    "\n",
    "    average_loss = total_loss / total_samples\n",
    "    print(f\"Train Loss: {average_loss:.3f}\")\n",
    "    return average_loss\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, x in enumerate(tqdm(dataloader)):\n",
    "            batch_size = x.shape[0]\n",
    "            total_samples += batch_size\n",
    "\n",
    "            x = x.flatten(start_dim=1)\n",
    "            x_pred, _ = model(x)\n",
    "            loss = loss_fn(x_pred, x)\n",
    "\n",
    "            total_loss += loss.item() * batch_size \n",
    "\n",
    "    average_loss = total_loss / total_samples\n",
    "    print(f\"Test Loss: {average_loss:.3f}\")\n",
    "    return average_loss\n",
    "\n",
    "model = DenseAutoencoder(window_size * len(feature_columns))\n",
    "epochs = 10\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop with noise injection to improve robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualConvAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, seq_len):\n",
    "        super(ResidualConvAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Conv1d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.MaxPool1d(2),  # Down-sampling\n",
    "            nn.Conv1d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(32, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ConvTranspose1d(64, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Conv1d(128, input_dim, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with skip connections.\n",
    "        \"\"\"\n",
    "        # Add a channel dimension (needed for Conv1d)\n",
    "        x = x.permute(0, 2, 1)  # Change shape from (batch_size, seq_len, features) to (batch_size, features, seq_len)\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        decoded = decoded.permute(0, 2, 1)  # Back to (batch_size, seq_len, features)\n",
    "        return decoded, encoded\n",
    "\n",
    "# Define the model\n",
    "input_dim = 89  # Number of features\n",
    "seq_len = 100   # Window size\n",
    "model = ResidualConvAutoencoder(input_dim, seq_len)\n",
    "\n",
    "# Print the model architecture\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "\n",
    "# Optimizer and Loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    train_loss = train_loop(train_dataloader, model, loss_fn, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, noise_factor=0.1, print_every=10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for _, x in enumerate(tqdm(dataloader)):\n",
    "        batch_size = x.shape[0]\n",
    "        total_samples += batch_size\n",
    "        # x = x.flatten(start_dim=1)\n",
    "        # Add noise to the input\n",
    "        noisy_x = x + noise_factor * torch.randn_like(x)\n",
    "        noisy_x = noisy_x.clamp(0., 1.)  # Ensure values stay within [0, 1]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_pred, _ = model(noisy_x)\n",
    "        \n",
    "        loss = loss_fn(x_pred, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * batch_size\n",
    "\n",
    "    average_loss = total_loss / total_samples\n",
    "    print(f\"Train Loss: {average_loss:.3f}\")\n",
    "    return average_loss\n",
    "\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, x in enumerate(tqdm(dataloader)):\n",
    "            batch_size = x.shape[0]\n",
    "            total_samples += batch_size\n",
    "\n",
    "            # x = x.flatten(start_dim=1)\n",
    "            x_pred, _ = model(x)\n",
    "            loss = loss_fn(x_pred, x)\n",
    "\n",
    "            total_loss += loss.item() * batch_size \n",
    "\n",
    "    average_loss = total_loss / total_samples\n",
    "    print(f\"Test Loss: {average_loss:.3f}\")\n",
    "    return average_loss\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_loss_evol = []\n",
    "test_loss_evol = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loss_evol.append(train_loop(train_dataloader, model, loss_fn, optimizer))\n",
    "    test_loss_evol.append(test_loop(test_dataloader, model, loss_fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((train_loss_evol), label = 'train')\n",
    "plt.plot((test_loss_evol), label = 'test')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Loss evolution')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "civil-426",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
